# ğŸ‹ Docker Exam Project: FastAPI Sentiment Test Pipeline
### Tested sentiment analysis API â€¢ Python-based tests â€¢ Reproducible â€¢ CI/CD-style â€¢ One container per test suite â€¢ Shared aggregated log

## ğŸ¯ What this project demonstrates
This repository implements a **Docker Compose test pipeline** for the sentiment analysis API image `datascientest/fastapi:1.0.0`.

âœ… **API container** exposed on host port 8000 (endpoints: `/status`, `/permissions`, `/v1/sentiment`, `/v2/sentiment`)  
âœ… **3 separate Python test containers** (one per suite) that validate:
- **Authentication** (`/permissions`)
- **Authorization** (`/v1/sentiment` vs `/v2/sentiment`)
- **Content** (positive/negative score checks for given sentences)

âœ… **Automatic sequential execution** via Compose `depends_on` conditions: API â†’ Authentication â†’ Authorization â†’ Content  
âœ… **LOG=1** support: all suites append into a single shared **`api_test.log`** (kept in `./shared/`).  
âœ… **`setup.sh`** runs the whole pipeline reproducibly and produces **`log.txt`** (submission artifact)  

---

## ğŸ­ Tech Stack
ğŸ‹ **Docker / Docker Compose** | ğŸ **Python 3.12** | ğŸŒ **requests** | âš™ï¸ **Makefile orchestration**

---

## ğŸ§  Engineering Notes (Beyond Requirements): Shared, reusable test framework

While the exam only requires â€œ3 test containers + test scripts + a shared logâ€, I deliberately invested extra effort to keep the solution **abstract, reusable, and maintainable**â€”so each suite only defines its **test cases** in a (mroe or less) DSL-like manner, while the execution (incl. assertions) + logging pipeline stays consistent across all suites.

### Whatâ€™s abstracted (and why it matters)
- **Central config loading** (`tests/_shared/config.py`)  
  All suites use the same env contract (`API_ADDRESS`, `API_PORT`, `LOG`, `LOG_PATH`, `HTTP_TIMEOUT`) so behavior is consistent across containers and host runs.

- **One generic request runner** (`tests/_shared/runner.py`)  
  A single function executes HTTP requests, validates status codes, and (only when required) validates sentiment score direction.  
  â†’ Suites donâ€™t duplicate request/validation logic.

- **Unified, deterministic logging** (`tests/_shared/logging.py`)  
  Consistent suite headers/footers + per-test formatting for stdout and (when `LOG=1`) a shared append-only log file.  
  â†’ The aggregated `api_test.log` stays readable and stable across runs.

- **Generic params handling** (`tests/_shared/params.py`)  
  `iter_params(...)` normalizes suite-specific param objects (dicts, dataclasses, NamedTuples, etc.) into `(key, value)` pairs for logging and request execution.  
  â†’ Each suite can model its test parameters however it wants without changing the logger/runner.

- **Shared types for clarity** (`tests/_shared/types.py`)  
  Common `TestCase` + `TestResult` structures keep the contract between suite definitions and the shared engine explicit.

### Result
Each suite module focuses on *only*:
- defining test cases (endpoint + params + expected outcomes)
- invoking the shared runner/logger
- returning an exit code suitable for CI/CD

Everything else (config, readiness waiting, request execution, output format, file logging) is handled once in `tests/_shared/`.

---

## ğŸ—ï¸ Architecture (Pipeline + Shared Log)

~~~text
                                (host)
                       ./shared/  +  ./log.txt
                          â–²               â–²
                          â”‚               â”‚  snapshot copy
         bind mount       â”‚               â””â”€ setup.sh / make snapshot-log
      ./shared:/shared    â”‚
                          â”‚
+-------------------------------------------------------------------------------+
|                          docker compose project                               |
|                                                                               |
|   +--------------------------+            +------------------------------+    |
|   |        API service        |<---------->|   internal network           |   |
|   |  datascientest/fastapi    |  HTTP      |   sentiment_net (DNS: api)   |   |
|   |  host 8000 -> :8000       |            +------------------------------+   |
|   +-------------+------------+                                                |
|                 ^                                                             |
|                 |  (all test suites call http://api:8000/...)                 |
|                 |                                                             |
|   +-------------+--------------------------------------------------------+    |
|   |                                                                      |    |
|   |   +-------------------+     +-------------------+     +----------------+  |
|   |   | auth_test (suite) | --> | authz_test (suite)| --> | content_test    | |
|   |   | /permissions      |     | /v1 + /v2 access  |     | /v1 + /v2 score | |
|   |   +---------+---------+     +---------+---------+     +--------+--------+ |
|   |             |                       |                        |            |
|   |             | append                | append                 | append     |
|   |             v                       v                        v            |
|   |       +--------------------------------------------------------------+    |
|   |       |          shared bind mount: ./shared : /shared               |    |
|   |       |          aggregated log:    /shared/api_test.log             |    |
|   |       +--------------------------------------------------------------+    |
|   |                                                                      |    |
|   +----------------------------------------------------------------------+    |
|                                                                               |
+-------------------------------------------------------------------------------+

Sequential order is enforced by docker-compose `depends_on` conditions:
- `auth_test` waits for `api` to start (service_started) + polls /status until ready
- `authz_test` starts only after `auth_test` finished successfully (service_completed_successfully)
- `content_test` starts only after `authz_test` finished successfully (service_completed_successfully)

All suites append into the same shared file: /shared/api_test.log
At the end, setup.sh snapshots it to ./log.txt (exam artifact).
~~~

---

## ğŸ“ Project Structure (high level)

~~~text
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ setup.sh
â”œâ”€â”€ README.md
â”œâ”€â”€ log.txt                  # exam artifact (snapshotted from ./shared/api_test.log)
â”œâ”€â”€ docs/
    â”œâ”€â”€ IMPLEMENTATION.md
â”œâ”€â”€ shared/
â”‚   â””â”€â”€ api_test.log         # aggregated suite logs (written by test containers when LOG=1)
â””â”€â”€ tests/
    â”œâ”€â”€ _shared/             # common helpers (config, logging, readiness, runner, types)
    â”œâ”€â”€ authentication/
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ test_authentication.py
    â”œâ”€â”€ authorization/
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ test_authorization.py
    â””â”€â”€ content/
        â”œâ”€â”€ Dockerfile
        â””â”€â”€ test_content.py
~~~

---

## ğŸš€ Quick Start (Exam Runner)

### 1) Run the full pipeline (build â†’ start â†’ test â†’ snapshot log â†’ cleanup)
~~~bash
./setup.sh
~~~

This will:
- reset to a clean state (containers/ports/logs)
- start the API + test containers
- run suites in order: **AUTHENTICATION â†’ AUTHORIZATION â†’ CONTENT**
- write the aggregated log to `./shared/api_test.log` (**exam requirement via LOG=1**)
- copy it to `./log.txt` (**submission artifact**)
- stop everything (rerun-safe)

---

## ğŸ” Manual API sanity checks (optional)

~~~bash
curl -s "http://localhost:8000/status"; echo
curl -s -o /dev/null -w "%{http_code}\n" "http://localhost:8000/docs"
~~~

---

## âœ… Most useful Make targets

- `make start-project` â€” start stack (detached) and build images
- `make stop-project` â€” stop stack (normal down)
- `make stop-all` â€” stop stack + remove orphans (quiet + idempotent)
- `make reset` â€” guaranteed clean state (stop-all + kill-api + free-port-8000 + reset-logs)
- `make logs` â€” follow logs for the whole stack
- `make logs-auth` / `make logs-authz` / `make logs-content` â€” print suite logs (tail)
- `make snapshot-log` â€” copy `./shared/api_test.log` â†’ `./log.txt`

---

## ğŸ§¾ Implementation log  

Instead of maintaining a separate `README_student.md`, this project keeps a single detailed build diary:

â¡ï¸ See **`docs/IMPLEMENTATION.md`** for step-by-step implementation notes, decisions, and commands:
-  [docs/IMPLEMENTATION.md](docs/IMPLEMENTATION.md)

---

## âš–ï¸ Notes on portability (UID/GID + bind mounts)

The test containers write into a **bind-mounted** folder (`./shared:/shared`).  
To avoid root-owned files on the host, the test services run as the host user:

- `setup.sh` exports `HOST_UID` and `HOST_GID`
- `docker-compose.yml` uses `user: "${HOST_UID}:${HOST_GID}"` for each test service

This keeps `./shared/api_test.log` writable and removable without `sudo`, and makes reruns deterministic.

---

## APPENDIX: Original Exam Brief (excerpt)

**Goal:** Build a small **CI/CD-style Docker Compose pipeline** that automatically tests a provided **sentiment analysis FastAPI** container image.

- API image: `datascientest/fastapi:1.0.0`
- Endpoints: `/status`, `/permissions`, `/v1/sentiment`, `/v2/sentiment`
- **Pipeline requirement:** Docker Compose must launch **4 containers total**:
  - 1Ã— API container
  - **3Ã— separate test containers** (**Authentication**, **Authorization**, **Content**) â€” one python test suite per container
- **Logging requirement:** When `LOG=1`, each suite must append its report into **`api_test.log`** (single aggregated file)
- **Expected test coverage:**
  - Authentication: `/permissions` returns **200** for `alice:wonderland` and `bob:builder`, and **403** for `clementine:mandarine`
  - Authorization: `bob` can use **v1 only**, `alice` can use **v1 and v2**
  - Content: using `alice`, sentences **"life is beautiful"** (positive score) and **"that sucks"** (negative score) must be validated for both **v1** and **v2**
- Final deliverables include: `docker-compose.yml`, Python test scripts, Dockerfiles, `setup.sh`, and a submission `log.txt` containing the aggregated results.

### âœ… Deliverables checklist (Exam Requirements)
- âœ… `docker-compose.yml` contains the **sequence of tests** (API + 3 suites)
- âœ… Python test files for **Authentication / Authorization / Content**
- âœ… Dockerfiles to build each test image
- âœ… `setup.sh` to build + launch the compose pipeline
- âœ… `log.txt` containing the aggregated logs (snapshotted from `./shared/api_test.log`)
- âœ… Optional remarks file: [docs/IMPLEMENTATION.md](docs/IMPLEMENTATION.md)